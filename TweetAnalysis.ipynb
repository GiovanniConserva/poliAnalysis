{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import vincent\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES:\n",
    "\n",
    "### - PREPROCESSING: \n",
    "    - FILTER STOPWORDS+COMMON WORDS\n",
    "    - RETRIEVE MOST COMMON TERMS, HASHTAGS (WITH ASSOCIATE COUNT)\n",
    "    - RETRIEVE CO-OCCURENT TERMS\n",
    "### - TIME SERIES ANALYSIS:\n",
    "    - REPRESENT HASHTAGS/TERMS TIMESERIES (WITH POSSIBILITY TO CHOOSE THE GRANULARITY)\n",
    "    - CLUSTER HASHTAGS/TERMS TIMESERIES USING K-MEANS\n",
    "### - VISUALIZATION:\n",
    "    - NATIVE VISUALIZATION OF TIME SERIES CLUSTERING\n",
    "    - EXPORT DATA TO PLOT GEOLOCATED GRAPHS AND TIME SERIES CHARTS ON JS LIBRARIES (LINKED IN THE COMMENTS)\n",
    "\n",
    "# TODO:\n",
    "### - IMPROVE PREPROCESSING \n",
    "    - FILTER MORE COMMON WORDS\n",
    "    - AUTOMATICALLY IMPROVE THE LIST OF NON INTERESTING WORDS FROM THE PREVIOUS ITERATIONS\n",
    "    - ADD STUDY OF BIGRAMS\n",
    "### - EXPLORE DIFFERENT MACHINE LEARNING ALGORITHMS FOR TIME SERIES\n",
    "    - STUDY MORE APPROPRIATE ALGORITHMS LIKE DTW\n",
    "### - STUDY GEOLOCATIONAL FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################DATABASE VARIABLES##################################\n",
    "db_name= \"immigration\"\n",
    "collection = \"immigration\"\n",
    "#TODO: automatically increment the list of stop words and hashtags\n",
    "personalized_excluded_words = ['example1','example2' ]\n",
    "personalized_excluded_hashtags = ['#example1','#example2']\n",
    "#if the dataset is too huge, these variables are used to chunk it from the starting index to the ending index\n",
    "dataset_start_index = 200000\n",
    "dataset_end_index = 300000\n",
    "\n",
    "################################TIME CLUSTERING VARIABLES##################################\n",
    "\n",
    "data_types= ['hashtag','words','accounts']\n",
    "#types of object to study\n",
    "chosen_type = data_types[0]\n",
    "#number of hashtags/terms/other to study\n",
    "objects_size= 30\n",
    "\n",
    "#to do: expand the color dictionary for more clusters \n",
    "plot_colors= {0:\"b\",1:\"g\",2:\"r\",3:\"c\",4:\"m\",5:\"y\",6:\"k\",7:\"w\",}\n",
    "\n",
    "from_filter= '2015-01-01'\n",
    "to_filter= '2016-10-01'\n",
    "\n",
    "time_span='10000Min' #scale of the time axis i.e. tweets are clustered each time_span fraction\n",
    "\n",
    "num_clusters= 5\n",
    "\n",
    "################################OUTPUT FILE NAMES##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client[db_name]\n",
    "coll= db[collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'immigration'), u'immigration')\n"
     ]
    }
   ],
   "source": [
    "print coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = coll.find()[dataset_start_index:dataset_end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print tweets.count(with_limit_and_skip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetsDf = pd.DataFrame(list(tweets)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print len(tweetsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tweet:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'RNC makes major hire to woo Latinos https://t.co/OiF8lfSf9O @GOP'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets= tweetsDf[\"text\"]\n",
    "print \"Example tweet:\"\n",
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@gioazzi', ':', 'example', '!', 'http://gioazzi.com', '#democrats', '#republicans']\n"
     ]
    }
   ],
   "source": [
    "##################################PREPROCESSING UTILITIES####################\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = \"@gioazzi: example!  http://gioazzi.com #democrats #republicans \"\n",
    "print(preprocess(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'Republican', u'immigration'), 10749), ((u'Republican', u'Trump'), 9496), ((u'Donald', u'Trump'), 6274), ((u'Immigration', u'Republican'), 5760), ((u'GOP', u'undocumented'), 5680)]\n"
     ]
    }
   ],
   "source": [
    "#TODO: escape emoticons errors\n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    "count_all = Counter()\n",
    "count_all_single = Counter()\n",
    "count_all_hash = Counter()\n",
    "count_all_terms = Counter()\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "stop = stopwords.words('english') + punctuation + personalized_excluded_words +personalized_excluded_hashtags #+com\n",
    "\n",
    "for index,row in tweetsDf.iterrows():\n",
    "\n",
    "    terms_stop = [term for term in preprocess(row[\"text\"]) if term not in stop]\n",
    "    terms_all = [term for term in preprocess(row[\"text\"]) if term not in stop]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all) \n",
    "    # Count terms only once, equivalent to Document Frequency\n",
    "    terms_single = set(terms_all)\n",
    "    count_all_single.update(terms_single)\n",
    "    # Count hashtags only\n",
    "    terms_hash = [term for term in preprocess(row[\"text\"]) \n",
    "                  if term.startswith('#') and term not in stop]\n",
    "    count_all_hash.update(terms_hash)\n",
    "    # Count terms only (no hashtags, no mentions)\n",
    "    terms_only = [term for term in preprocess(row[\"text\"]) \n",
    "                  if term not in stop and\n",
    "                  not term.startswith(('#', '@'))] \n",
    "                  # mind the ((double brackets))\n",
    "                  # startswith() takes a tuple (not a list) if \n",
    "                  # we pass a list of inputs\n",
    "    count_all_terms.update(terms_only)    \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1\n",
    "                                \n",
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************DOCUMENT FREQUENCY****************\n",
      "All\n",
      "[(u'Republican', 35502), (u'GOP', 31822), (u'immigration', 19439), (u'Trump', 16032), (u'immigrants', 10813), (u'undocumented', 9835), (u'Immigration', 9371), (u'The', 8817), (u'Latino', 8230), (u'via', 7781), (u'\\u2026', 6976), (u'Undocumented', 6545), (u'Latinos', 6510), (u'#GOP', 6400), (u'amp', 6090)]\n",
      "Only hashtags\n",
      "[(u'#GOP', 6429), (u'#tcot', 3961), (u'#immigration', 3269), (u'#gop', 2160), (u'#Republican', 1521), (u'#p2', 1426), (u'#teaparty', 1222), (u'#UniteBlue', 1152), (u'#ocra', 1077), (u'#Immigration', 1055), (u'#Trump', 1029), (u'#sgp', 1018), (u'#GOPDebate', 922), (u'#Election2016', 876), (u'#ImmigrationReform', 845)]\n",
      "Only terms\n",
      "[(u'Republican', 35949), (u'GOP', 32509), (u'immigration', 19657), (u'Trump', 17491), (u'immigrants', 10944), (u'undocumented', 9912), (u'The', 9860), (u'Immigration', 9495), (u'Latino', 8464), (u'via', 7788), (u'amp', 7446), (u'\\u2026', 7226), (u'\\u2019', 6841), (u'Latinos', 6757), (u'Undocumented', 6611)]\n"
     ]
    }
   ],
   "source": [
    "print \"*************DOCUMENT FREQUENCY****************\"\n",
    "print \"All\"\n",
    "print(count_all_single.most_common(15))\n",
    "print \"Only hashtags\"\n",
    "print(count_all_hash.most_common(15))\n",
    "print \"Only terms\"\n",
    "print(count_all_terms.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7756</th>\n",
       "      <td>#GOP</td>\n",
       "      <td>6429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9260</th>\n",
       "      <td>#tcot</td>\n",
       "      <td>3961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>#immigration</td>\n",
       "      <td>3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>#gop</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>#Republican</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  count\n",
       "7756          #GOP   6429\n",
       "9260         #tcot   3961\n",
       "3670  #immigration   3269\n",
       "8262          #gop   2160\n",
       "8645   #Republican   1521"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_all_dict= dict(count_all_hash)\n",
    "df = pd.DataFrame.from_dict(count_all_dict, orient='index').reset_index()\n",
    "df.columns= [\"word\",\"count\"]\n",
    "df = df.sort(['count'],ascending=False)\n",
    "df=df[:objects_size]\n",
    "df.to_json(\"Frequency.json\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TO PLOT: \n",
    "Run the server:\n",
    "python -m http.server 8888 # Python 3\n",
    "python -m SimpleHTTPServer 8888 # Python 2\n",
    "\n",
    "in chart.html uncomment this line: parse(\"term_freq.json\");\n",
    "\n",
    "open\n",
    "http://localhost:8888/chart.html\n",
    "'''\n",
    "\n",
    "#prepare data to plot a barchar of the most frequent terms\n",
    "word_freq = count_all_terms.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  count\n",
      "7756          #GOP   6429\n",
      "9260         #tcot   3961\n",
      "3670  #immigration   3269\n",
      "8262          #gop   2160\n",
      "8645   #Republican   1521\n"
     ]
    }
   ],
   "source": [
    "#prepare data to plot timeseries of some terms/hashtags in the dates_dict\n",
    "print df.head(5)\n",
    "'''\n",
    "TO PLOT: \n",
    "Run the server:\n",
    "python -m http.server 8000 # Python 3\n",
    "python -m SimpleHTTPServer 8000 # Python 2\n",
    "\n",
    "in chart.html uncomment this line: parse(\"time_chart.json\");\n",
    "\n",
    "open\n",
    "http://localhost:8000/chart.html\n",
    "'''\n",
    "#to study custom hashtag initializate the dates_dict as in the comment below:\n",
    "#dates_dict = {'#immigration':[], '#GOP':[] } \n",
    "dates_dict = dict((word,[]) for word in df[\"word\"][:30])\n",
    "#dates_dict = take(200, dates_dict.iteritems())\n",
    "#print dates_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'#ImmigrationReform': [], u'#sgp': [], u'#GOPDebate': [], u'#teaparty': [], u'#GOP': [], u'#Conservative': [], u'#US': [], u'#Latinos': [], u'#tlot': [], u'#Election2016': [], u'#TNTweeters': [], u'#ccot': [], u'#gop': [], u'#Trump2016': [], u'#Trump': [], u'#news': [], u'#ocra': [], u'#Latino': [], u'#republican': [], u'#Immigration': [], u'#immigrants': [], u'#Not1More': [], u'#tcot': [], u'#UniteBlue': [], u'#immigration': [], u'#Republican': [], u'#CIR': [], u'#politics': [], u'#p2': [], u'#undocumented': []}\n"
     ]
    }
   ],
   "source": [
    "print dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for index,row in tweetsDf.iterrows(): \n",
    "    if chosen_type== 'hashtag':\n",
    "        terms = [term for term in preprocess(row['text']) if term.startswith('#')]\n",
    "    if chosen_type== 'words':\n",
    "        terms = [term for term in preprocess(row['text']) if not term.startswith(('#', '@'))]\n",
    "    if chosen_type== 'accounts':\n",
    "        terms = [term for term in preprocess(row['text']) if term.startswith('@')]\n",
    "\n",
    "    # track when the hashtag is mentioned\n",
    "    for key in dates_dict:\n",
    "        \n",
    "        if key in terms:\n",
    "            dates_dict[key].append(row['raw']['created_at'])\n",
    "             \n",
    "# a list of \"1\" to count the hashtags\n",
    "ones= {}\n",
    "idx={}\n",
    "dataframes={}\n",
    "per_minute={}\n",
    "for key in dates_dict:\n",
    "    \n",
    "    ones[key] = [1]*len(dates_dict[key])\n",
    "    # the index of the series\n",
    "    idx[key] = pd.DatetimeIndex(dates_dict[key])\n",
    "    # the actual series (at series of 1s for the moment)\n",
    "    dataframes[key] = pd.Series(ones[key], index=idx[key])\n",
    "                                                          \n",
    "    # Resampling / bucketing\n",
    "    per_minute[key] = dataframes[key].resample(time_span, how='sum').fillna(0)\n",
    "print \"done\"\n",
    "#print per_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     #CIR  #Conservative  #Election2016  #GOP  #GOPDebate  \\\n",
      "2016-03-06 08:00:00     0             22              4    72         156   \n",
      "2016-03-13 06:40:00    24             38             14   224           2   \n",
      "2016-03-20 05:20:00    12             64             14   472           0   \n",
      "2016-03-27 04:00:00    60             54             32   510           4   \n",
      "2016-04-03 02:40:00    10             40              8   302           0   \n",
      "\n",
      "                     #Immigration  #ImmigrationReform  #Latino  #Latinos  \\\n",
      "2016-03-06 08:00:00            18                   2        6        12   \n",
      "2016-03-13 06:40:00            66                   2       20        40   \n",
      "2016-03-20 05:20:00            88                  24       30        36   \n",
      "2016-03-27 04:00:00            88                   6       52        56   \n",
      "2016-04-03 02:40:00            54                   2       32        40   \n",
      "\n",
      "                     #Not1More      ...        #news  #ocra  #p2  #politics  \\\n",
      "2016-03-06 08:00:00          2      ...            6      8   10          0   \n",
      "2016-03-13 06:40:00          0      ...            6     36   18         14   \n",
      "2016-03-20 05:20:00          2      ...           18     40   24         14   \n",
      "2016-03-27 04:00:00          0      ...           18     28   30         20   \n",
      "2016-04-03 02:40:00          0      ...           14     32   10          8   \n",
      "\n",
      "                     #republican  #sgp  #tcot  #teaparty  #tlot  #undocumented  \n",
      "2016-03-06 08:00:00           22    10     28          2      4              0  \n",
      "2016-03-13 06:40:00           22    18     20          8     10              0  \n",
      "2016-03-20 05:20:00           10    12    196         20     18              0  \n",
      "2016-03-27 04:00:00           22     8    200         26     14              6  \n",
      "2016-04-03 02:40:00            2    20    158          4      6              0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# all the data together\n",
    "match_data = per_minute\n",
    "# we need a DataFrame, to accommodate multiple series\n",
    "all_matches = pd.DataFrame(data=match_data) #,index=per_minute['#'].index\n",
    "# Resampling \n",
    "all_matches = all_matches.resample(time_span, how='sum').fillna(0)\n",
    "#selecting interval to study \n",
    "all_matches= all_matches.ix[from_id:to_id]    \n",
    "    \n",
    "#prepare data to plot on external libraries\n",
    "time_chart = vincent.Line(all_matches[list(match_data.keys())])\n",
    "time_chart.axis_titles(x='Time', y='Freq')\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart.to_json('time_chart.json')\n",
    "all_matches.to_csv('time_chart.csv')\n",
    "print all_matches.head(5)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1 2 0 2 0 0 2 0 2 0 0 2 2 2 0 2 3 2 2 2 2 2 2 4 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "###########################CLUSTERING###################\n",
    "#labels identify the cluster\n",
    "matches_np= np.array(all_matches).transpose()\n",
    "kmeans= KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(matches_np)\n",
    "#centroids= kmeans.cluster_centers_\n",
    "labels= kmeans.labels_\n",
    "#print centroids\n",
    "print labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster 0', ' color b', u'#Conservative', u'#Immigration', u'#Latino', u'#Latinos', u'#Republican', u'#Trump', u'#Trump2016', u'#gop']\n",
      "['cluster 1', ' color g', u'#GOP']\n",
      "['cluster 2', ' color r', u'#CIR', u'#Election2016', u'#GOPDebate', u'#ImmigrationReform', u'#Not1More', u'#TNTweeters', u'#US', u'#UniteBlue', u'#ccot', u'#immigrants', u'#news', u'#ocra', u'#p2', u'#politics', u'#republican', u'#sgp', u'#teaparty', u'#tlot', u'#undocumented']\n",
      "['cluster 3', ' color c', u'#immigration']\n",
      "['cluster 4', ' color m', u'#tcot']\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "clusters=[]\n",
    "for i in xrange(0,num_clusters):\n",
    "    clusters.append([\"cluster \"+ str(i), \" color \"+ plot_colors[i]])\n",
    "    \n",
    "i=0\n",
    "for el in all_matches.columns.values:    \n",
    "    #print el + \" color: \"+ plot_colors[labels[i]]+ \" cluster \"+ str(labels[i])\n",
    "    clusters[labels[i]].append(el)\n",
    "    i=i+1\n",
    "for el in clusters:\n",
    "    print el\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in xrange(1,len(matches_np)):\n",
    "    y= matches_np[index]\n",
    "    x = np.array(all_matches.index)\n",
    "    #print x.shape\n",
    "    #y = np.random.randint(100, size=x.shape)\n",
    "    plt.plot(x,y, color= plot_colors[labels[index]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare data to plot geolocated graphs\n",
    "\n",
    "#TO PLOT: http://geojson.io/#map=2/20.0/0.0\n",
    "\n",
    "geo_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "for index,row in tweetsDf.iterrows():\n",
    "    if row['raw']['coordinates']:\n",
    "        geo_json_feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": row['raw']['coordinates'],\n",
    "            \"properties\": {\n",
    "                \"text\": row['text'],\n",
    "                \"created_at\": row['raw']['created_at']\n",
    "            }\n",
    "        }\n",
    "        geo_data['features'].append(geo_json_feature)\n",
    " \n",
    "# Save geo data\n",
    "with open('geo_data.json', 'w') as fout:\n",
    "    fout.write(json.dumps(geo_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBigrams: not used yet\\nfrom nltk import bigrams \\nterms_bigram = bigrams(terms_stop)\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Bigrams: not used yet\n",
    "from nltk import bigrams \n",
    "terms_bigram = bigrams(terms_stop)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
